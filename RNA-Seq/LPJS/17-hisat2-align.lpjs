#!/bin/sh -e

##########################################################################
#   Description:
#       Run hisat2 aligner on each RNA sample.
#
#   Dependencies:
#       Requires hisat2 index.  Run after *-hisat2-index.lpjs.
#
#       All necessary tools are assumed to be in PATH.  If this is not
#       the case, add whatever code is needed here to gain access.
#       (Adding such code to your .bashrc or other startup script is
#       generally a bad idea since it's too complicated to support
#       every program with one environment.)
#       
#   History:
#   Date        Name        Modification
#   2021-11-24  Jason Bacon Begin
##########################################################################

# One job per sample
#lpjs jobs 18
# 3 processors for hisat2, 1 for samtools sort, all must be on the same node
#lpjs processors-per-job 4
#lpjs threads-per-process processors-per-job
# From lpjs peak-mem:
#lpjs pmem-per-processor 1516MiB
#lpjs log-dir Logs/17-hisat2-align
##############################################################################
# Update PATH on a chimeric cluster (multiple operating systems used for
# compute nodes)
#
# The PATH used by the package manager that installed LPJS (/usr/local for
# FreeBSD ports, usually /usr/pkg or /*/pkg for pkgsrc), is automatically
# prepended to the default PATH.  This is overridden by "#lpjs path", so
# if we use it, we must add all directories ourselves.
#
# Add the default non-priveleged pkgsrc prefix used by auto-pkgsrc-setup.
#
# Caution: Different versions of rsync behave differently with respect
# to creating path components at the destination.  Newer rsync requires
# --mkpath while older ones included with macOS and RHEL do not support
# this flag. Set path to use pkgsrc rsync in ~/Pkgsrc/pkg or /*/pkg.
#lpjs path ~/Pkgsrc/pkg/bin:/opt/pkg/bin:/usr/pkg/bin:/usr/local/bin:/usr/bin:/bin
#lpjs pull-command rsync -r --copy-links %h:%s/Reference . && rsync --mkpath -r %h:%s/Results/16-hisat2-index Results/16-hisat2-index && rsync --mkpath -r %h:%s/Results/04-trim/\*sample%i-\*.zst Results/04-trim
#lpjs push-command rsync --mkpath -r %w/Results/17-hisat2-align %h:%s/Results

##############################################################################
# Align with hisat2, which can handle splice junctions in RNA reads

# Set a default value for testing outside the LPJS environment
: ${LPJS_ARRAY_INDEX:=1}
# Set to at least 2
: ${LPJS_THREADS_PER_PROCESS:=4}

# Adjust this and #lpjs jobs above to rerun limited samples
# E.g. to rerun just sample 9, use 1 job, sample = array_index 1 + 8:
#   #lpjs jobs 1
#   sample=$(($LPJS_ARRAY_INDEX + 8))
sample=$LPJS_ARRAY_INDEX

# Document software versions used for publication
uname -a
hisat2 --version
pwd

input_dir=Results/04-trim
output_dir=Results/17-hisat2-align
index_dir=Results/16-hisat2-index

# Make sure $tmpdir has enough space for several fastq files of ~2GB each
# *and* samtools temporary files (comparable in size to fastq inputs).
# Use compute node's local disk if possible.  Samtools sort uses this for
# many large temporary files, so using networked storage will
# hammer the cluster network and file servers.
if hostname | grep albacore; then
    # /usr/tmp is on a big partition on our albacore cluster's compute nodes
    tmpdir=/usr/tmp
else
    tmpdir=$(pwd)
fi

# hisat2 can't yet read zstd inputs.
# https://github.com/DaehwanKimLab/hisat2/issues/412
# It also cannot read input from a pipe (performs seeks?)
# If your trimmed fastq files are in zstd
# format, this will convert to gzip format.
# Convert zstd to gz rather than raw to reduce NFS/IO load from compute nodes
# --fast minimizes CPU usage in exchange for larger files
zst1=$(echo $input_dir/*sample${sample}-*-R1.fastq.zst)
zst2=$(echo $input_dir/*sample${sample}-*-R2.fastq.zst)
base1=$(basename $zst1)
base2=$(basename $zst2)
gz1=$tmpdir/${base1%.zst}.gz
gz2=$tmpdir/${base2%.zst}.gz
# Run both at the same time, since we allocated 4 cores anyway
# These are CPU-bound, so disk will not usually be a bottleneck
# Don't try to reuse the .gz files, they may be incomplete from canceled jobs
printf "$zst1 -> $gz1, $zst2 -> $gz2 for hisat2...\n"
zstdcat $zst1 | gzip --fast --stdout > $gz1 &
zstdcat $zst2 | gzip --fast --stdout > $gz2
wait    # .gz files must be complete before we start

genome=$(Reference/genome-filename.sh)
gz_base=$(basename $gz1)
bam=${gz_base%-R*}.bam

# Show exact commands below in stderr output
set -x

cd $output_dir
hisat2 --threads $(($LPJS_THREADS_PER_PROCESS - 1)) -x ../16-hisat2-index/$genome \
    -1 $gz1 -2 $gz2 | samtools sort -T $tmpdir > $bam

# Not sure how helpful multithreading is here, but since we allocated
# the cores for hisat2 above, might as well use them.  It's not worth
# running a separate job for indexing.
samtools index -@ $LPJS_PROCESSORS_PER_JOB -c $bam
